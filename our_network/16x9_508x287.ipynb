{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive is mounted.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# This will display the contents of your Google Drive's root directory if mounted,\n",
    "# otherwise, it will throw an error.\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive is mounted.\")\n",
    "except:\n",
    "    print(\"Google Drive is not mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_WIDTH = 508\n",
    "IMAGE_HEIGHT = 287\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 200\n",
    "FEATURES_CRITIC = 64\n",
    "FEATURES_GEN = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_dir = r\"/content/drive/My Drive/GAN - Data Projekt/\"\n",
    "\n",
    "dataroot = \"/content/drive/My Drive/GAN - Data Projekt/Billeder(Trimmet)/Billeder (gode)\"  # dental radiography data/Good\"\n",
    "\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "                               transforms.Grayscale(num_output_channels=1),  # Converts RGB to greyscale\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5), (0.5))  # Use a single value for mean and std\n",
    "                           ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "Mon May 20 12:16:13 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0              52W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "CPU Information:\n",
      "Architecture:             x86_64\n",
      "  CPU op-mode(s):         32-bit, 64-bit\n",
      "  Address sizes:          46 bits physical, 48 bits virtual\n",
      "  Byte Order:             Little Endian\n",
      "CPU(s):                   12\n",
      "  On-line CPU(s) list:    0-11\n",
      "Vendor ID:                GenuineIntel\n",
      "  Model name:             Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "    CPU family:           6\n",
      "    Model:                85\n",
      "    Thread(s) per core:   2\n",
      "    Core(s) per socket:   6\n",
      "    Socket(s):            1\n",
      "    Stepping:             7\n",
      "    BogoMIPS:             4400.44\n",
      "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 cl\n",
      "                          flush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc re\n",
      "                          p_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3\n",
      "                           fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand\n",
      "                           hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp \n",
      "                          ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm\n",
      "                           mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw av\n",
      "                          x512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capab\n",
      "                          ilities\n",
      "Virtualization features:  \n",
      "  Hypervisor vendor:      KVM\n",
      "  Virtualization type:    full\n",
      "Caches (sum of all):      \n",
      "  L1d:                    192 KiB (6 instances)\n",
      "  L1i:                    192 KiB (6 instances)\n",
      "  L2:                     6 MiB (6 instances)\n",
      "  L3:                     38.5 MiB (1 instance)\n",
      "NUMA:                     \n",
      "  NUMA node(s):           1\n",
      "  NUMA node0 CPU(s):      0-11\n",
      "Vulnerabilities:          \n",
      "  Gather data sampling:   Not affected\n",
      "  Itlb multihit:          Not affected\n",
      "  L1tf:                   Not affected\n",
      "  Mds:                    Vulnerable; SMT Host state unknown\n",
      "  Meltdown:               Not affected\n",
      "  Mmio stale data:        Vulnerable\n",
      "  Reg file data sampling: Not affected\n",
      "  Retbleed:               Vulnerable\n",
      "  Spec rstack overflow:   Not affected\n",
      "  Spec store bypass:      Vulnerable\n",
      "  Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swa\n",
      "                          pgs barriers\n",
      "  Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI:\n",
      "                           Vulnerable (Syscall hardening enabled)\n",
      "  Srbds:                  Not affected\n",
      "  Tsx async abort:        Vulnerable\n"
     ]
    }
   ],
   "source": [
    "# Check GPU information\n",
    "print(\"GPU Information:\")\n",
    "!nvidia-smi\n",
    "\n",
    "# Check CPU information\n",
    "print(\"\\nCPU Information:\")\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    #print(BATCH_SIZE, C, H, W)\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    #print(f\"Real shape: {real.shape}, Fake shape: {fake.shape}\")\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"celeba_wgan_gp.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, gen, disc):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    gen.load_state_dict(checkpoint['gen'])\n",
    "    disc.load_state_dict(checkpoint['disc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the Generator class as previously defined\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Initial input is Z: (channels_noise x 1 x 1)\n",
    "            nn.ConvTranspose2d(channels_noise, features_g * 16, 4, 1, 0, bias=False),  # output: (features_g * 16) x 4 x 4\n",
    "            nn.BatchNorm2d(features_g * 16),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 16, features_g * 8, 4, 2, 1, bias=False),  # output: (features_g * 8) x 8 x 8\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 8, features_g * 4, 4, 2, 1, bias=False),  # output: (features_g * 4) x 16 x 16\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, 4, 2, 1, bias=False),  # output: (features_g * 2) x 32 x 32\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 2, features_g, 4, 2, 1, bias=False),  # output: (features_g) x 64 x 64\n",
    "            nn.BatchNorm2d(features_g),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g, features_g // 2, 4, 2, 1, bias=False),  # output: (features_g // 2) x 128 x 128\n",
    "            nn.BatchNorm2d(features_g // 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g // 2, features_g // 4, 4, 2, 1, bias=False),  # output: (features_g // 4) x 256 x 256\n",
    "            nn.BatchNorm2d(features_g // 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g // 4, features_g // 8, (4, 2), (1, 2), (1, 1), bias=False),  # output: (features_g // 8) x 256 x 512\n",
    "            nn.BatchNorm2d(features_g // 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g // 8, channels_img, (33, 1), (1, 1), (1, 1), bias=False),  # Fine-tune to exact 288 x 512\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, features_d, 4, 2, 1, bias=False),  # 64 x 144 x 256\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(features_d, features_d * 2, 4, 2, 1, bias=False),  # 128 x 72 x 128\n",
    "            nn.InstanceNorm2d(features_d * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(features_d * 2, features_d * 4, 4, 2, 1, bias=False),  # 256 x 36 x 64\n",
    "            nn.InstanceNorm2d(features_d * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(features_d * 4, features_d * 8, 4, 2, 1, bias=False),  # 512 x 18 x 32\n",
    "            nn.InstanceNorm2d(features_d * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(features_d * 8, features_d * 16, 4, 2, 1, bias=False),  # 1024 x 9 x 16\n",
    "            nn.InstanceNorm2d(features_d * 16, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(features_d * 16, features_d * 32, 4, 2, 1, bias=False),  # 2048 x 4 x 8\n",
    "            nn.InstanceNorm2d(features_d * 32, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(features_d * 32, 1, (4, 7), (1, 1), 0, bias=False),  # 1 x 1 x 1\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the generator and critic\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "# Initialize the optimizers\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer_real = SummaryWriter()\n",
    "writer_fake = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        # Train Critic\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            )\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "        # Train Generator\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Log the losses to TensorBoard\n",
    "        if batch_idx % 10 == 0:\n",
    "            writer_real.add_scalar(\"Critic Loss\", loss_critic.item(), global_step=epoch * len(loader) + batch_idx)\n",
    "            writer_fake.add_scalar(\"Generator Loss\", loss_gen.item(), global_step=epoch * len(loader) + batch_idx)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise).detach().cpu()\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                writer_real.add_image(\"Real Images\", img_grid_real, global_step=epoch * len(loader) + batch_idx)\n",
    "                writer_fake.add_image(\"Fake Images\", img_grid_fake, global_step=epoch * len(loader) + batch_idx)\n",
    "\n",
    "        # Update the progress bar\n",
    "        loop.set_postfix(loss_critic=loss_critic.item(), loss_gen=loss_gen.item())\n",
    "\n",
    "    print(f\"Completed Epoch {epoch+1}/{NUM_EPOCHS}.\")  # Confirms the completion of the current epoch\n",
    "\n",
    "    # Save the model checkpoint after 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"gen_state_dict\": gen.state_dict(),\n",
    "            \"critic_state_dict\": critic.state_dict(),\n",
    "            \"gen_optimizer\": opt_gen.state_dict(),\n",
    "            \"critic_optimizer\": opt_critic.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "    elif epoch == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"gen_state_dict\": gen.state_dict(),\n",
    "            \"critic_state_dict\": critic.state_dict(),\n",
    "            \"gen_optimizer\": opt_gen.state_dict(),\n",
    "            \"critic_optimizer\": opt_critic.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "# Close the TensorBoard writers\n",
    "writer_real.close()\n",
    "writer_fake.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
